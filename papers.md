## Repository of papers: 

- ~**[Evaluating Verifiability in Generative Search Engines](https://arxiv.org/abs/2304.09848)** by Nelson F. Liu, Tianyi Zhang, Percy Liang~
- ~**[On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research](https://arxiv.org/abs/2304.12397)** by Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker~
- ~**[Dissecting Recall of Factual Associations in Auto-Regressive Language Models](https://arxiv.org/abs/2304.14767)** by Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson~
- ~**[Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks](https://arxiv.org/abs/2305.10160)** by Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg~
- ~**[Evaluating the Factual Consistency of Large Language Models Through Summarization](https://arxiv.org/abs/2211.08412)** by Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin Raffel~
- ~**[Moving beyond “algorithmic bias is a data problem”](https://www.sciencedirect.com/science/article/pii/S2666389921000611)** by Sara Hooker~
- ~**[DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature](https://arxiv.org/abs/2301.11305)** by Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn~
- ~**[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)** by Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein~
- ~**[Auditing large language models: a three-layered approach](https://arxiv.org/abs/2302.08500)** by Jakob Mökander, Jonas Schuett, Hannah Rose Kirk, Luciano Floridi~
- ~**[Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://arxiv.org/abs/2210.17546)** by Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini~
- ~**[Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646)** by Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang~
- ~**[Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML](https://arxiv.org/abs/2305.18615)** by Giada Pistilli, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell~
- ~**[On the Reliability of Watermarks for Large Language Models](https://arxiv.org/abs/2306.04634)** by John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein~
- ~**[Debiased Fine-Tuning for Vision-language Models by Prompt Regularization](https://arxiv.org/abs/2301.12429)** by Beier Zhu, Yulei Niu, Saeil Lee, Minhoe Hur, Hanwang Zhang~
- ~**[Grokking of Hierarchical Structure in Vanilla Transformers](https://arxiv.org/abs/2305.18741)** by Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher Manning~
- ~**[RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)** by Bo Peng et al.~
- ~**[Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621)** Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei~
- ~**[Backpack Language Models](https://arxiv.org/abs/2305.16765)** by John Hewitt, John Thickstun, Christopher Manning, Percy Liang~
- ~**[Organizational Governance of Emerging Technologies: AI Adoption in Healthcare](https://arxiv.org/abs/2304.13081)** by Jee Young Kim et al.~
- ~**[The Gradient of Generative AI Release: Methods and Considerations](https://arxiv.org/abs/2302.04844)** by Irene Solaiman~
- ~**[Predictability and Surprise in Large Generative Models](https://dl.acm.org/doi/abs/10.1145/3531146.3533229)** by Deep Ganguli et al.~
- ~**[Towards Climate Awareness in NLP Research](https://aclanthology.org/2022.emnlp-main.159/)** by Daniel Hershcovich, Nicolas Webersinke, Mathias Kraus, Julia Bingler, Markus Leippold~
<!--Extending Context Length-->  
- ~**[LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)** by Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei~
- ~**[Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs//2306.15595)** by Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian~
- ~**[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)** by Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang~
<!--Scaling-->
- ~**[No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models](https://arxiv.org/pdf/2307.06440.pdf)** by Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, Matt J. Kusner1~
<!--Decoding Innovations-->  
- ~**[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)** by Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan~
- ~**[Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](https://arxiv.org/abs/2307.15337)** Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang~
<!--Crowdsurcing Annotations-->  
- ~**[Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks](https://arxiv.org/abs/2306.07899)** by Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West~
- ~**[LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs](https://arxiv.org/pdf/2307.10168.pdf)** by Tongshuang Wu et al.~
- ~**[Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design](https://arxiv.org/abs/2304.00815)** by Valentina Pyatkin, Frances Yung, Merel C.J. Scholman, Reut Tsarfaty, Ido Dagan, Vera Demberg~
- ~**[Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies](https://openreview.net/forum?id=eYlLlvzngu)** by Gati Aher, Rosa I. Arriaga, Adam Tauman Kalai~
<!--Privacy and Security in LLMs-->  
- ~**[What Does it Mean for a Language Model to Preserve Privacy?](https://dl.acm.org/doi/pdf/10.1145/3531146.3534642)** by Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, Florian Tramèr~
- ~**[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)** by Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson~
<!--Transformer ML-->  
- ~**[A Study on Transformer Configuration and Training Objective](https://arxiv.org/abs/2205.10505)** by Fuzhao Xue et al.~
- ~**[Auto-Regressive Next-Token Predictors are Universal Learners](https://arxiv.org/abs/2309.06979)** by Eran Malach~
- ~**[Transformers as Support Vector Machines](https://arxiv.org/abs/2308.16898)** by Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet Oymak~
- ~**[Stabilizing Transformer Training by Preventing Attention Entropy Collapse](https://arxiv.org/abs/2303.06296)** by Shuangfei Zhai et al.~
<!--ICL and Emergent Abilities-->
- ~**[Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004)** by Rylan Schaeffer, Brando Miranda, Sanmi Koyejo~
- ~**[Are Emergent Abilities in Large Language Models just In-Context Learning?](https://arxiv.org/abs/2309.01809)** by Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych~
<!--Instruction Tuning and Aligning with Humans Preferences -->
- ~**[On the Exploitability of Instruction Tuning](https://arxiv.org/abs/2306.17194)** by Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein~
- ~**[Exploring the Benefits of Training Expert Language Models over Instruction Tuning](https://arxiv.org/abs/2302.03202)** by Joel Jang et al.~
<!--Jailbreaking and Defenses-->
- ~**[Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/2307.02483)** by Alexander Wei, Nika Haghtalab, Jacob Steinhardt~
- ~**[How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373)** by Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi~
- ~**[Baseline Defenses for Adversarial Attacks Against Aligned Language Models](https://arxiv.org/abs/2309.00614v2)** by Neel Jain et al.~
- ~**[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)** by Evan Hubinger et al.~
- ~**[FLIRT: Feedback Loop In-context Red Teaming](https://arxiv.org/abs/2308.04265)** by Ninareh Mehrabi et al.~
<!--Copyright/Privacy Issues, Data Attribution, Model Unlearning -->  
- ~**[Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks](https://arxiv.org/abs/2310.07879)** by Hao-Ping Lee et al.~
- ~**[Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://arxiv.org/abs/2310.17884)** by Niloofar Mireshghallah et al.~
<!-- Prompting-->
- ~**[Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting](https://aclanthology.org/2024.eacl-long.159/)** by Tilman Beck, Hendrik Schuff, Anne Lauscher, Iryna Gurevych~
- ~**[Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting](https://arxiv.org/abs/2310.11324)** by Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr~
<!--Privacy, IT/FT-->
- ~**[Proving membership in LLM pretraining data via data watermarks](https://arxiv.org/abs/2402.10892)** by Johnny Tian-Zheng Wei, Ryan Yixiang Wang, Robin Jia~
- ~**[Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://aclanthology.org/2023.emnlp-main.291.pdf)** by Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, Jacob Andreas~
- ~**[The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717)** by Arnav Gudibande et al.~

<!--ICLR 2024-->
#### ICLR 2024:
- ~**[Proving Test Set Contamination in Black Box Language Models](https://arxiv.org/abs/2310.17623)** by Yonatan Oren et el.~
- ~**[Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/abs/2310.03693)** by Xiangyu Qi et al.~
- ~**[Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511)** by Akari Asai et al.~
- ~**[LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models](https://arxiv.org/abs/2309.14393)** by Ahmad Faiz et al.~
- ~**[Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](https://arxiv.org/abs/2305.09955)** by Shangbin Feng et al.~
- ~**[BooookScore: A systematic exploration of book-length summarization in the era of LLMs](https://arxiv.org/abs/2310.00785)** by Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer~
- **[ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://arxiv.org/abs/2310.04564)** by Iman Mirzadeh et al.
- **[Self-Alignment with Instruction Backtranslation](https://arxiv.org/abs/2308.06259)** by Xian Li et al.
- **[Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors](https://arxiv.org/abs/2310.02980)** by Ido Amos, Jonathan Berant, Ankit Gupta
- **[Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://arxiv.org/abs/2309.17410)** by Vaidehi Patil, Peter Hase, Mohit Bansal
- **[Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298)** by Robin Staab
- **[Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions](https://arxiv.org/abs/2311.00233)** by Taehyeon Kim, Joonkee Kim, Gihun Lee, Se-Young Yun
- **[At Which Training Stage Does Code Data Help LLMs Reasoning?](https://arxiv.org/abs/2309.16298)** by Yingwei Ma et el.
- **[Time Travel in LLMs: Tracing Data Contamination in Large Language Models](https://arxiv.org/abs/2308.08493)** by Shahriar Golchin, Mihai Surdeanu

- **[Unbiased Watermark for Large Language Models](https://openreview.net/forum?id=uWVC5FVidc)** by Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang
- **[Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities Of Language Models With Hypothesis Refinement](https://openreview.net/forum?id=bNt7oajl2a)** by Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren
- **[In-Context Pretraining: Language Modeling beyond Document Boundaries](https://openreview.net/forum?id=LXVswInHOo)** by Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Gergely Szilvasy, Rich James, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis
- **[Catastrophic Jailbreak of Open-Source LLMs](https://openreview.net/forum?id=r42tSSCHPh)** by Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen
- **[Overthinking the Truth: How Language Models Process False Demonstrations](https://openreview.net/forum?id=Tigr1kMDZy)** by Danny Halawi, Jean-Stanislas Denain∗, and Jacob Steinhardt
- **[Controlled Text Generation via Language Model Arithmetic](https://openreview.net/forum?id=SLw9fp4yI6)** by Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner, Martin Vechev
  
#### Others:
- **[TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://aclanthology.org/2022.acl-long.229.pdf/)** by Stephanie Lin, Jacob Hilton and Owain Evans.
- **[Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162)** by Boyi Wei et al.
<!--Privacy Issues-->
- **[Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://arxiv.org/abs/2309.11765)** by Xinyu Tang et al.
<!--Uncertainity-->
- **[Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty](https://arxiv.org/abs/2401.06730)** by Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Maarten Sap
<!--Limitations of IT/FT-->
- **[Pretraining Language Models with Human Preferences](https://arxiv.org/abs/2302.08582)** by Tomasz Korbak et al.
- **[A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119)** by Sreyan Ghosh et al.
- **[The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning](https://arxiv.org/abs/2312.01552)** by Bill Yuchen Lin et al.
- **[Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)** by Nicholas Carlini et al.
<!--Auditing-->
- **[Automatically Auditing Large Language Models via Discrete Optimization](https://arxiv.org/abs/2303.04381)** by Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt
- **[Bring Your Own Data! Self-Supervised Evaluation for Large Language Models](https://arxiv.org/abs/2306.13651v1)** by Neel Jain et el.
- **[Causality-aware Concept Extraction based on Knowledge-guided Prompting](https://arxiv.org/abs/2305.01876)** by Siyu Yuan, Deqing Yang, Jinxi Liu, Shuyu Tian, Jiaqing Liang, Yanghua Xiao, Rui Xie
- **[Compositional Generalization without Trees using Multiset Tagging and Latent Permutations](https://arxiv.org/abs/2305.16954)** by Matthias Lindemann, Alexander Koller, Ivan Titov
- **[A New Direction in Stance Detection: Target-Stance Extraction in the Wild](https://aclanthology.org/2023.acl-long.560/)** by Yingjie Li, Krishna Garg, Cornelia Caragea
- **[Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection](https://arxiv.org/abs/2306.00765)** by Erik Arakelyan, Arnav Arora, Isabelle Augenstein
<!--Reproducility/ Consistency/ Fairness in Evaluations-->
- **[REFORMS: Reporting Standards for Machine Learning Based Science](https://arxiv.org/abs/2308.07832)** by Sayash Kapoor et al.
- **[Predictive Multiplicity in Probabilistic Classification](https://arxiv.org/abs/2206.01131)** by Jamelle Watson-Daniels, David C. Parkes, Berk Ustun
- **[Is My Prediction Arbitrary? The Confounding Effects of Variance in Fair Classification Benchmarks](https://arxiv.org/abs/2301.11562)** by A. Feder Cooper et al.
- **[Troubling Trends in Machine Learning Scholarship](https://arxiv.org/abs/1807.03341)** by Zachary C. Lipton, Jacob Steinhardt
<!--Memorisation and Generalisation-->  
- **[Explaining grokking through circuit efficiency](https://arxiv.org/abs/2309.02390)** by Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar
<!--Multi-linguality-->  
- **[Multilingual Event Extraction from Historical Newspaper Adverts](https://arxiv.org/abs/2305.10928)** by Nadav Borenstein, Natália da Silva Perez, Isabelle Augenstein
- **[When Does Translation Require Context? A Data-driven, Multilingual Exploration](https://aclanthology.org/2023.acl-long.36/)** by Patrick Fernandes, Kayo Yin, Emmy Liu, André Martins, Graham Neubig
- **[Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models](https://openreview.net/forum?id=SP6w4sVCyp)** by Phillip Rust, Anders Søgaard
<!--Efficient Training, Fine-tuning, Inference-->  
- **[Large Language Models Can Be Easily Distracted by Irrelevant Context](https://openreview.net/forum?id=JSZmoN03Op)** by Freda Shi et al.
- **[Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach](https://arxiv.org/abs/2209.06995)** by Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, Chao Zhang
- **[Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs//2306.09782)** by Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu
- **[Stack More Layers Differently: High-Rank Training Through Low-Rank Updates](https://arxiv.org/abs/2307.05695)** Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, Anna Rumshisky
- **[Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)** by Greg Yang et al.
- **[The case for 4-bit precision: k-bit Inference Scaling Laws](https://openreview.net/forum?id=i8tGb1ab1j)** by Tim Dettmers, Luke Zettlemoyer
<!--Explanation and Rationalization-->  
- **[Faithfulness Tests for Natural Language Explanations](https://arxiv.org/abs/2305.18029)** by Pepa Atanasovaet el.
- **[Towards Trustworthy Explanation: On Causal Rationalization](https://openreview.net/forum?id=fvTgh4MNUV)** by Wenbo Zhang, TONG WU, Yunlong Wang, Yong Cai, Hengrui Cai
- **[Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection](https://aclanthology.org/2023.acl-long.22/)** by Christopher Clarke, Matthew Hall, Gaurav Mittal, Ye Yu, Sandra Sajeev, Jason Mars, Mei Chen
<!--Bias and Fairness-->  
- **[A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models](https://aclanthology.org/2023.acl-long.878/)** by Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram
- **[Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting](https://arxiv.org/abs/2110.05367)** by Zahra Fatemi, Chen Xing, Wenhao Liu, Caimming Xiong, Zahra Fatemi
- **[Data Feedback Loops: Model-driven Amplification of Dataset Biases](https://openreview.net/forum?id=8JXMDw2xGa)** by Rohan Taori, Tatsunori Hashimoto
- **[The Bias Amplification Paradox in Text-to-Image Generation](https://yanaiela.github.io/papers/bias-amplification-paradox.pdf)** by Preethi Seshadri, Sameer Singh, Yanai Elazar
- **[FairPrism: Evaluating Fairness-Related Harms in Text Generation](http://users.umiacs.umd.edu/~hal/docs/daume23fairprism.pdf)** by Eve Fleisig et el.
- **["I wouldn’t say offensive but...": Disability-Centered Perspectives on Large Language Models](https://research.google/pubs/pub52358/)** by Vinitha Gadiraju, Shaun Kane, Sunipa Dev, Alex Taylor, Ding Wang, Emily Denton, Robin Brewer
- **[Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models](https://aclanthology.org/2023.acl-long.84.pdf)** by Myra Cheng, Esin Durmus, Dan Jurafsky
- **[Whose Opinions Do Language Models Reflect?](https://openreview.net/forum?id=7IRybndMLU)** by Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto

#### EMNLP 2024
- [Prompts have evil twins](https://arxiv.org/abs/2311.07064) by Rimon Melamed, Lucas Hurley McCabe, Tanay Wakhare, Yejin Kim, H. Howie Huang, Enric Boix-Adserà
- [ORPO: Monolithic Preference Optimization without Reference Model](https://aclanthology.org/2024.emnlp-main.626/) by Jiwoo Hong, Noah Lee, James Thorne
- [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://aclanthology.org/2024.emnlp-main.248/) by Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo
- [Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts](https://arxiv.org/abs/2406.12845) by Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang
- [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](https://aclanthology.org/2024.emnlp-main.444/) by Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig
- [Humans or LLMs as the Judge? A Study on Judgement Bias](https://aclanthology.org/2024.emnlp-main.474/) by Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang
- [Arcee's MergeKit: A Toolkit for Merging Large Language Models](https://arxiv.org/abs/2403.13257) by Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz
- [Knowledge Conflicts for LLMs: A Survey](https://aclanthology.org/2024.emnlp-main.486/) by Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu
- [MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents](https://arxiv.org/abs/2404.10774) by Liyan Tang, Philippe Laban, Greg Durrett
- [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019) by Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu

#### ACL 2024
- [M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://aclanthology.org/2024.findings-acl.137/) by Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
- [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373) by Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi
- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159) by Soldani et al. 
- [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066) by Dai et al. 
- [Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827) by Üstün et al. 
- [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) by Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, Dong Yu
- [Benchmarking Retrieval-Augmented Generation for Medicine](https://arxiv.org/abs/2402.13178) by Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang


#### Papers on test time compute
- [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393) by Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto
- [O1 Replication Journey: A Strategic Progress Report -- Part 1](https://arxiv.org/abs/2410.18982) by Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, Pengfei Liu
- [Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](https://arxiv.org/abs/2407.21787) by Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, Azalia Mirhoseini
- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314) by Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
- [From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Model](https://arxiv.org/abs/2406.16838) by Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) by Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo
- [Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models](https://arxiv.org/abs/2408.00724) by Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang
- [Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning](https://openreview.net/forum?id=A6Y7AqlzLW) by Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar
- [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935) by Peiyi Wang, Lei Li, Zhihong Shao, R.X. Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, Zhifang Sui

#### NeurIPS 24

##### 13th March

- [Large Language Models Must Be Taught to Know What They Don't Know](https://arxiv.org/abs/2406.08391) Kapoor et al
- [xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token](https://arxiv.org/abs/2405.13792) Cheng et al
- [Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/abs/2407.16607) Hayase et al
- [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905) Tian et al


#### Some popular papers on twitter / from arxiv-feed

- [Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences](https://arxiv.org/abs/2404.12272)
- [Do generative video models understand physical principles?](https://arxiv.org/abs/2501.09038)
- [Large Language Models Reflect the Ideology of their Creators](https://arxiv.org/abs/2410.18417)
- [Uncovering Gaps in How Humans and LLMs Interpret Subjective Language](https://www.arxiv.org/abs/2503.04113)

#### 27th March
- [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)
- [Byte Latent Transformer: Patches Scale Better Than Tokens](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)
- [Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657)
- [LLM-Generated Black-box Explanations Can Be Adversarially Helpful](https://arxiv.org/abs/2405.06800)
